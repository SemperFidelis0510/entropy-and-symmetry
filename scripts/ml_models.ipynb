{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2fa0753-3183-4778-af7d-bab77743ef14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "from utils import print_progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9dfdaaf-7338-400f-b0f1-02329586c987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_labels = ['plain nature', 'detailed nature', 'Agriculture', 'villages', 'city']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ea9cc58-c789-4a75-bbc5-24f6b0e3722e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, queries):\n",
    "        N = queries.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "        # Split the embedding into self.head different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        scores = scores / (self.head_dim ** 0.5)\n",
    "        attention = torch.nn.functional.softmax(scores, dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# Define the Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, 2 * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2 * embed_size, embed_size),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.batch_norm1 = nn.BatchNorm1d(embed_size)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(embed_size)\n",
    "\n",
    "    def forward(self, value, key, query):\n",
    "        attention = self.attention(value, key, query)\n",
    "\n",
    "        # Reshape tensor before BatchNorm1d\n",
    "        reshaped_attention = attention.view(-1, attention.size(-1))\n",
    "\n",
    "        x = self.batch_norm1(self.norm1(reshaped_attention))\n",
    "        x = x.view(attention.size())  # Reshape back to original size\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "\n",
    "        # Reshape tensor before second BatchNorm1d\n",
    "        reshaped_forward = forward.view(-1, forward.size(-1))\n",
    "\n",
    "        out = self.batch_norm2(self.norm2(reshaped_forward))\n",
    "        out = out.view(forward.size())  # Reshape back to original size\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.encoder1 = nn.Embedding(256, 512)\n",
    "        self.encoder2 = nn.Embedding(256, 512)\n",
    "        self.transformer1 = TransformerBlock(embed_size=512, heads=8)\n",
    "        self.transformer2 = TransformerBlock(embed_size=512, heads=8)\n",
    "        self.classifier = nn.Linear(1024, num_classes)\n",
    "        self.possible_labels = all_labels\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.encoder1(x1)\n",
    "        x2 = self.encoder2(x2)\n",
    "        x1 = self.transformer1(x1, x1, x1)\n",
    "        x2 = self.transformer2(x2, x2, x2)\n",
    "        # Padding x1 to match the sequence length of x2\n",
    "        if x1.size(1) < x2.size(1):\n",
    "            padding = torch.zeros(x1.size(0), x2.size(1) - x1.size(1), x1.size(2)).to(x1.device)\n",
    "            x1 = torch.cat((x1, padding), dim=1)\n",
    "        # Padding x2 to match the sequence length of x1\n",
    "        elif x2.size(1) < x1.size(1):\n",
    "            padding = torch.zeros(x2.size(0), x1.size(1) - x2.size(1), x2.size(2)).to(x2.device)\n",
    "            x2 = torch.cat((x2, padding), dim=1)\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=2)\n",
    "        x = self.classifier(x[:, 0, :])\n",
    "        return x\n",
    "\n",
    "    def train_model(self, dataset, epochs=100):\n",
    "        loss = None\n",
    "        entropies = [torch.tensor(d['entropies'], dtype=torch.long) for d in dataset]\n",
    "        dwt_entropies = [torch.tensor(d['dwt'], dtype=torch.long) for d in dataset]\n",
    "        labels = [self.possible_labels.index(d['label']) for d in dataset]\n",
    "        entropies = torch.stack(entropies)\n",
    "        dwt_entropies = torch.stack(dwt_entropies)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        train_data = TensorDataset(entropies, dwt_entropies, labels)\n",
    "        train_loader = DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data1, data2, target) in enumerate(train_loader):\n",
    "                self.train()\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data1, data2)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=1)\n",
    "                optimizer.step()\n",
    "            scheduler.step(loss.item())\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, numbers1, numbers2):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            numbers1 = torch.tensor(numbers1, dtype=torch.long).unsqueeze(0)\n",
    "            numbers2 = torch.tensor(numbers2, dtype=torch.long).unsqueeze(0)\n",
    "            output_ = self(numbers1, numbers2)\n",
    "            predicted_label_idx = torch.argmax(output_, dim=1).item()\n",
    "            return self.possible_labels[predicted_label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c5a5ae-7ed8-4ec9-8d8e-88d37b8add02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiLabelImageClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLabelImageClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc1 = nn.Linear(128 * 6 * 6, 256)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "        self.possible_labels = ['plain nature', 'detailed nature', 'Agriculture', 'villages', 'city']\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.bn1(self.conv1(x)))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.bn2(self.conv2(x)))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.relu(self.bn3(self.conv3(x)))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (6, 6))\n",
    "        x = x.view(-1, 128 * 6 * 6)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x \n",
    "    \n",
    "    def train_model(self, dataset, epochs=100, batch_size=64, lr=0.01):\n",
    "        t = time.time()\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        n = len(train_loader)\n",
    "        all_labels = self.possible_labels\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print_progress_bar('Batches processed', 0, n, start_time=t)\n",
    "            for batch_idx, batch in enumerate(train_loader):\n",
    "                data = batch['image'].float()\n",
    "                target = batch['label']\n",
    "                # print(f\"\\nData shape: {data.shape}, Target shape: {len(target)}, Target: {target}\")\n",
    "\n",
    "                target = torch.tensor([one_hot_encode(t, all_labels) for t in target], dtype=torch.float32)\n",
    "                \n",
    "                # Debugging: Print shapes\n",
    "                # print(f\"\\nData shape: {data.shape}, Target shape: {target.shape}\")\n",
    "\n",
    "                self.train()\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                print_progress_bar('Batches processed', batch_idx+1, n, start_time=t)\n",
    "            \n",
    "\n",
    "            print(f\"\\n Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, image, threshold=0.5):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            image = image.unsqueeze(0).float()\n",
    "            output_ = self(image)\n",
    "            output_ = (output_ > threshold).float()\n",
    "            return [label for idx, label in enumerate(self.possible_labels) if output_[0, idx] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103cc14b-6931-4043-9bb3-a68aecfdb0c3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim, dwt_input_dim, num_classes):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.layer3 = nn.Linear(64, num_classes)\n",
    "        self.dwt_layer = nn.Linear(dwt_input_dim, 128) if dwt_input_dim else None\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.possible_labels = all_labels\n",
    "\n",
    "    def forward(self, x, dwt_x=None):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.dropout(x)\n",
    "        if dwt_x is not None and self.dwt_layer:\n",
    "            dwt_x = F.relu(self.dwt_layer(dwt_x))\n",
    "            x += dwt_x\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.softmax(self.layer3(x), dim=1)\n",
    "        return x\n",
    "\n",
    "    def train_model(self, dataset, epochs=100):\n",
    "        loss = None\n",
    "        entropies = [torch.tensor(d['entropies'], dtype=torch.float) for d in dataset]\n",
    "        labels = [torch.tensor([1 if l in d['label'] else 0 for l in self.possible_labels], dtype=torch.float) for d in\n",
    "                  dataset]\n",
    "        entropies = torch.stack(entropies)\n",
    "        labels = torch.stack(labels)\n",
    "        train_data = TensorDataset(entropies, labels)\n",
    "        train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                self.train()\n",
    "                optimizer.zero_grad()\n",
    "                output = self(data)\n",
    "                loss = criterion(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
    "\n",
    "    def predict(self, entropies, dwt=None):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            entropies = torch.tensor(entropies, dtype=torch.float).unsqueeze(0)\n",
    "            if dwt is not None and self.dwt_layer:\n",
    "                dwt = torch.tensor(dwt, dtype=torch.float).unsqueeze(0)\n",
    "            else:\n",
    "                dwt = None\n",
    "            output_ = self(entropies, dwt)\n",
    "            predicted_label_idx = torch.argmax(output_, dim=1).item()\n",
    "            return self.possible_labels[predicted_label_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0348e1f3-af32-42d7-be79-65a33a4a36fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_json(path, test_part, model_type=\"SimpleMLP\"):\n",
    "    with open(path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    dataset = []\n",
    "\n",
    "    i = 0\n",
    "    n = len(metadata)\n",
    "    for entry in metadata:\n",
    "        i += 1\n",
    "        if model_type == \"ImageClassifier\":\n",
    "            image_path = entry['path']\n",
    "            image = cv2.imread(image_path)\n",
    "            if image is None:\n",
    "                print(f\"Could not read image at {image_path}\")\n",
    "                continue\n",
    "            resized_image = cv2.resize(image, (224, 224))\n",
    "            resized_image = torch.tensor(resized_image, dtype=torch.float32).permute(2, 0, 1) / 255.0\n",
    "            \n",
    "            label = entry['label']\n",
    "            if isinstance(label, str):\n",
    "                label = [label]\n",
    "            dataset.append({'image': resized_image, 'label': label})\n",
    "            \n",
    "        else:\n",
    "            entropies = [s['result'] for s in entry['entropy_results'] if s['method'] != 'dwt']\n",
    "            dwt_entropies = next((s['result'][:9] for s in entry['entropy_results'] if s['method'] == 'dwt'), None)\n",
    "            dataset.append({'entropies': entropies, 'dwt': dwt_entropies, 'label': entry['label']})\n",
    "        \n",
    "        print_progress_bar('Processed entry', i, n)\n",
    "\n",
    "    print(f\"\\nLength of dataset: {len(dataset)}\")\n",
    "\n",
    "    if isinstance(test_part, float):\n",
    "        i = int(test_part * len(dataset))\n",
    "    elif isinstance(test_part, str):\n",
    "        i = int(test_part)\n",
    "    else:\n",
    "        raise ValueError(\"Incompatible format for 'test_part'.\")\n",
    "\n",
    "    test_set = dataset[-i:]\n",
    "    dataset = dataset[:-i]\n",
    "\n",
    "    if model_type != \"ImageClassifier\":\n",
    "        input_dim = len(dataset[0]['entropies'])\n",
    "        dwt_input_dim = len(dataset[0]['dwt'])\n",
    "    else:\n",
    "        input_dim = None\n",
    "        dwt_input_dim = None\n",
    "\n",
    "    num_classes = len(all_labels)\n",
    "    \n",
    "    # Debugging print statements\n",
    "    print(f\"Total number of entries in the dataset: {len(dataset)}\")\n",
    "    print(f\"Total number of entries in the test set: {len(test_set)}\")\n",
    "    # print(f\"Sample entry from dataset: {dataset[0]}\")\n",
    "    # print(f\"Sample entry from test set: {test_set[0]}\")\n",
    "    \n",
    "    if model_type != \"ImageClassifier\":\n",
    "        print(f\"Input dimension for entropies: {input_dim}\")\n",
    "        print(f\"Input dimension for dwt: {dwt_input_dim}\")\n",
    "    else:\n",
    "        print(\"Model type is ImageClassifier, skipping input dimensions.\")\n",
    "\n",
    "    print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "    return dataset, test_set, input_dim, dwt_input_dim, num_classes\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_set, model_type=\"SimpleMLP\"):\n",
    "    stats = {'test_samples': 0, 'right_predictions': 0}\n",
    "    for test in test_set:\n",
    "        stats['test_samples'] += 1\n",
    "        if model_type == \"ImageClassifier\":\n",
    "            predicted_label = model.predict(test['image'])\n",
    "        else:\n",
    "            predicted_label = model.predict(test['entropies'], test['dwt'])\n",
    "        \n",
    "        if predicted_label == test[\"label\"]:\n",
    "            stats['right_predictions'] += 1\n",
    "            print(f'Predicted label: {predicted_label}.  Real label: {test[\"label\"]}. Prediction correct!')\n",
    "        else:\n",
    "            print(f'Predicted label: {predicted_label}.  Real label: {test[\"label\"]}. False prediction.')\n",
    "\n",
    "    stats['success_rate'] = 100 * stats['right_predictions'] / stats['test_samples']\n",
    "    print(f\"{stats['right_predictions']} samples out of {stats['test_samples']} were predicted correctly.\\n\"\n",
    "          f\"The model's success rate is: {stats['success_rate']}%\")\n",
    "\n",
    "\n",
    "def one_hot_encode(labels, all_labels_):\n",
    "    if not isinstance(labels, list):\n",
    "        labels = [labels]\n",
    "    num_labels = len(labels)\n",
    "    return [1/num_labels if label in labels else 0 for label in all_labels_]\n",
    "\n",
    "\n",
    "def custom_collate(batch):\n",
    "    images = torch.stack([item['image'] for item in batch])\n",
    "    labels = [item['label'] for item in batch]\n",
    "    return {'image': images, 'label': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af0dc385-f45d-4909-84f6-02b913a93d92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = \"ImageClassifier\" # Choose one amongst (\"SimpleMLP\", \"Transformer\", \"ImageClassifier\").\n",
    "path = \"../processed/results/entropy_results.json\"\n",
    "test_part = \"100\"\n",
    "epochs = 50\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb5c67e4-9df4-46b7-a573-36f8f1b558cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed entry: ██████████████████████████████████████████████████ | 100.0% Complete 4437/4437 instances.\n",
      "Length of dataset: 4437\n",
      "Total number of entries in the dataset: 4337\n",
      "Total number of entries in the test set: 100\n",
      "Model type is ImageClassifier, skipping input dimensions.\n",
      "Number of classes: 5\n",
      "Dataset processed.\n"
     ]
    }
   ],
   "source": [
    "dataset, test_set, input_dim, dwt_input_dim, num_classes = process_json(path, test_part, model_type)\n",
    "print('Dataset processed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586dfa1-0b13-4165-88f9-f7732ca9707e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 05:20\n",
      " Epoch 1, Loss: 0.314670592546463\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 10:35\n",
      " Epoch 2, Loss: 0.3372536301612854\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 15:56\n",
      " Epoch 3, Loss: 0.30121394991874695\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 21:15\n",
      " Epoch 4, Loss: 0.3019165098667145\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 26:29\n",
      " Epoch 5, Loss: 0.2572595179080963\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 31:47\n",
      " Epoch 6, Loss: 0.3073972761631012\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 37:07\n",
      " Epoch 7, Loss: 0.31755322217941284\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 42:36\n",
      " Epoch 8, Loss: 0.3340013325214386\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 48:05\n",
      " Epoch 9, Loss: 0.2632160484790802\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 53:30\n",
      " Epoch 10, Loss: 0.2879614233970642\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 59:00\n",
      " Epoch 11, Loss: 0.34423717856407166\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 64:27\n",
      " Epoch 12, Loss: 0.3010072112083435\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 69:57\n",
      " Epoch 13, Loss: 0.303018182516098\n",
      "Batches processed: ██████████████████████████████████████████████████ | 100.0% Complete 68/68 instances | Time: 75:47\n",
      " Epoch 14, Loss: 0.2179529368877411\n",
      "Batches processed: ██------------------------------------------------ | 5.9% Complete 4/68 instances | Time: 76:08"
     ]
    }
   ],
   "source": [
    "if model_type == \"SimpleMLP\":\n",
    "    model = SimpleMLP(input_dim, dwt_input_dim, num_classes)   \n",
    "elif model_type == \"Transformer\":\n",
    "    model = Classifier(num_classes)    \n",
    "elif model_type == \"ImageClassifier\":\n",
    "    model = MultiLabelImageClassifier(num_classes)    \n",
    "else:\n",
    "    print(\"Invalid model type\")    \n",
    "    \n",
    "model.train_model(dataset, epochs=epochs, batch_size=batch_size)\n",
    "print('Model trained.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e9483a-9995-4dea-88e9-37a7c664efa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_model(model, test_set, model_type=\"ImageClassifier\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
